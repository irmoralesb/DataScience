{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Steps\n",
    "\n",
    "## Data Load\n",
    "\n",
    "* Use pandas to load csv\n",
    "\n",
    "## Data analysis\n",
    "\n",
    "* Chart and Plotting to see data shape\n",
    "* Identify missing data, invalid values\n",
    "* Dataframe Description\n",
    "* Dataframe.describe()\n",
    "* Unique values: species = iris_ds_file[\"Species\"].unique()\n",
    "\n",
    "## Preprocessing\n",
    "\n",
    "* Replace invalid values with NaN\n",
    "    * df.column.replace(0, np.nan, inplace=True)\n",
    "    * df[df == '?'] = np.nan\n",
    "    * df.isnull().sum()\n",
    "    * df.dropna()\n",
    "* Imputer\n",
    "    * sklearn.preprocessing.Imputer\n",
    "    \n",
    "    ```python\n",
    "        imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "        imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "        #axis = 0 -> columns & 1 -> rows\n",
    "        imp.fit(X)\n",
    "        X = imp.transform(X)\n",
    "\n",
    "    ```\n",
    "* Transform categories into numeric. Target Categories for classification problems\n",
    "    * sklearn.preprocessing.LabelEncoder\n",
    "\n",
    "    ```python\n",
    "        species_lbl = LabelEncoder()\n",
    "        iris_ds_file[\"Species_code\"] = species_lbl.fit_transform(iris_ds_file[\"Species\"])\n",
    "    ```\n",
    "\n",
    "* Dummies, for regression problems and we need to transform categorical features into numeric features\n",
    "\n",
    "    ```python\n",
    "        iris_ds_dummies = pd.get_dummies(iris_ds_file, drop_first=True)\n",
    "        print(\"Iris Columns: {}\".format(iris_ds_dummies.columns))\n",
    "    ```\n",
    "* Scaling Data\n",
    "    ```python\n",
    "        from sklearn.preprocessing import scale\n",
    "        X_scale = scale(X)\n",
    "    ```\n",
    "\n",
    "## ML Algorithm Selection\n",
    "\n",
    "\n",
    "## Creating Model\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "* sklearn.model_selection.train_test_split\n",
    "* sklearn.model_selection.GridSearchCV - Few parameters to configure\n",
    "    ```python\n",
    "        from sklearn.model_selection import GridSearchCV\n",
    "        param_grid = {'n_neighbors': np.arange(1,50)}\n",
    "        knn = KNeighborsClassifier()\n",
    "        knn_cv = GridSearchCV(knn, param_grid=param_grid, cv=5)\n",
    "        knn_cv.fit(X_train, y_train)\n",
    "        print('Best parameters: {}'.format(knn_cv.best_params_))\n",
    "        print('Best Score {}'.format(knn_cv.best_score_))\n",
    "    ```\n",
    "* sklearn.model_selection.RandomizedSearchCV - Many parameters to configure\n",
    "    ```python\n",
    "        # Import necessary modules\n",
    "        from scipy.stats import randint\n",
    "        from sklearn.tree import DecisionTreeClassifier\n",
    "        from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "        # Setup the parameters and distributions to sample from: param_dist\n",
    "        param_dist = {\"max_depth\": [3, None],\n",
    "                    \"max_features\": randint(1, 9),\n",
    "                    \"min_samples_leaf\": randint(1, 9),\n",
    "                    \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "        # Instantiate a Decision Tree classifier: tree\n",
    "        tree = DecisionTreeClassifier()\n",
    "\n",
    "        # Instantiate the RandomizedSearchCV object: tree_cv\n",
    "        tree_cv = RandomizedSearchCV(tree, param_dist, cv=5)\n",
    "\n",
    "        # Fit it to the data\n",
    "        tree_cv.fit(X, y)\n",
    "\n",
    "        # Print the tuned parameters and score\n",
    "        print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\n",
    "        print(\"Best score is {}\".format(tree_cv.best_score_))\n",
    "    ```\n",
    "\n",
    "### Models\n",
    "\n",
    "#### Classification\n",
    "\n",
    "* sklearn.neighbors.KNeighborsClassifier\n",
    "* sklearn.tree.DecisionTreeClassifier\n",
    "* sklearn.svm.SVC\n",
    "\n",
    "#### Regression\n",
    "\n",
    "#### Execution\n",
    "\n",
    "* ML model execution\n",
    "\n",
    "    ```python\n",
    "        knn = KNeighborsClassifier(n_neighbors=neighbors)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_predict = knn.predict(X_test)\n",
    "    ```\n",
    "\n",
    "* Pipelines - link steps in the preprocessing and execution steps\n",
    "    * sklearn.pipeline.Pipeline\n",
    "\n",
    "    ```python\n",
    "        from sklearn.preprocessing import Imputer\n",
    "        from sklearn.pipeline import Pipeline\n",
    "        from sklearn.svm import SVC\n",
    "\n",
    "        # Setup the pipeline steps: steps\n",
    "        steps = [('imputation', Imputer(missing_values='NaN', strategy='most_frequent', axis=0)),\n",
    "                ('SVM', SVC())]\n",
    "\n",
    "        # Create the pipeline: pipeline\n",
    "        pipeline = Pipeline(steps)\n",
    "\n",
    "        # Create training and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Fit the pipeline to the train set\n",
    "        pipeline.fit(X_train, y_train)\n",
    "\n",
    "        # Predict the labels of the test set\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "\n",
    "        # Compute metrics\n",
    "        print(classification_report(y_test, y_pred))\n",
    "    ```\n",
    "\n",
    "## Evaluating Model\n",
    "\n",
    "### Metrics\n",
    "\n",
    "* Accuracy = (tp + tn) / [tp + tn + fp + fn]\n",
    "* Precision = tp / (tp + fp)\n",
    "* Recall = tp / (tp + fn)\n",
    "* F1 Score = 2 [(precision * recall) / (precision + recall)]\n",
    "\n",
    "High Accuracy = Predecting correctly the expected value\n",
    "High Precision = We are classifiying correctly the object of interest\n",
    "High Recall = We are classifying correctly when it is not the object of interest\n",
    "\n",
    "```python\n",
    "    accuracy knn.score(X_test, y_test)\n",
    "```\n",
    "\n",
    "### Best Parameter for the model\n",
    "\n",
    "#### Manual approach\n",
    "\n",
    "Identify the best k for test and training data\n",
    "\n",
    "```python\n",
    "    neighbors = np.arange(1,9)\n",
    "    train_accuracy = np.empty(len(neighbors))\n",
    "    test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "    for idx, k_value in enumerate(neighbors):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k_value)\n",
    "        knn.fit(X_train, y_train)\n",
    "        train_accuracy[idx] = knn.score(X_train, y_train)\n",
    "        test_accuracy[idx] = knn.score(X_test, y_test)\n",
    "\n",
    "    #Plotting\n",
    "    plt.title(\"k-NN: Varying Number of Neighbors\")\n",
    "    plt.plot(neighbors, test_accuracy, label= 'Test accuracy', )\n",
    "    plt.plot(neighbors, train_accuracy, label='Train accuracy')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Number of neighbors\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    # plt.axes([0.96,1,0,9])\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "#### Cross Validation\n",
    "\n",
    "```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    # Let's use k=5\n",
    "    knn = KNeighborsClassifier(n_neighbors=5)\n",
    "    cv_result = cross_val_score(knn, X_train, y_train, cv=5)\n",
    "    print(\"Cross-validation result: {}\".format(cv_result))\n",
    "    print(\"Cross-validation mean: {}\".format(np.mean(cv_result)))\n",
    "```\n",
    "\n",
    "#### Confusion Matrix\n",
    "\n",
    "```python\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(confusion_matrix(y_test, y_predict))\n",
    "    print(\"Classification Report\")\n",
    "    print(classification_report(y_test, y_predict))\n",
    "```\n",
    "Confusion Matrix\n",
    "[[10  0  0]\n",
    " [ 0 12  1]\n",
    " [ 0  0  7]]\n",
    "Classification Report\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      1.00      1.00        10\n",
    "           1       1.00      0.92      0.96        13\n",
    "           2       0.88      1.00      0.93         7\n",
    "\n",
    "    accuracy                           0.97        30\n",
    "   macro avg       0.96      0.97      0.96        30\n",
    "weighted avg       0.97      0.97      0.97        30\n",
    "\n",
    "#### ROC (ONLY BINARY CASES)\n",
    "\n",
    "> Checks the probability\n",
    "\n",
    "```python\n",
    "    from sklearn.metrics import roc_curve\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Probability first line\n",
    "\n",
    "    y_predict_prob = knn.predict_proba(X_test)[:,1]\n",
    "    print(type(y_predict_prob))\n",
    "    print(type(y_test))\n",
    "\n",
    "    #Generate ROC curve values: fpr, tpr, thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_predict_prob)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr)\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.show()\n",
    "```\n",
    "\n",
    "#### Area under the curve (AUC)\n",
    "\n",
    "Larger area under the curve is a better model\n",
    "\n",
    "```python\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    roc_auc_score(y_test, y_predict_prob)\n",
    "    # The closest to 1 the better\n",
    "```\n",
    "\n",
    "##### AUC using Cross-validation\n",
    "```python\n",
    "cv_scores = cross_val_score(knn, X_train, y_train, cv=5, scoring='roc_auc')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
