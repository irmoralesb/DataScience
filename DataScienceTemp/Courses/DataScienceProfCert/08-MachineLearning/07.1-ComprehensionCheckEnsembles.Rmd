---
title: "Comprehension Check: Ensembles"
output: github_document
---

For these exercises we are going to build several machine learning models for the mnist_27 dataset and then build an ensemble. Each of the exercises in this comprehension check builds on the last.  


## Q1

Use the training set to build a model with several of the models available from the caret package. We will test out all of the following models in this exercise:

```{r}
models <- c("glm", "lda",  "naive_bayes",  "svmLinear", 
                "gamboost",  "gamLoess", "qda", 
                "knn", "kknn", "loclda", "gam",
                "rf", "ranger",  "wsrf", "Rborist", 
                "avNNet", "mlp", "monmlp",
                "adaboost", "gbm",
                "svmRadial", "svmRadialCost", "svmRadialSigma")
```

We have not explained many of these, but apply them anyway using train with all the default parameters. You will likely need to install some packages. Keep in mind that you will probably get some warnings. Also, it will probably take a while to train all of the models - be patient!

Run the following code to train the various models:

```{r}
library(caret)
library(dslabs)
set.seed(1)
data("mnist_27")

fits <- lapply(models, function(model){ 
	print(model)
	train(y ~ ., method = model, data = mnist_27$train)
}) 
    
names(fits) <- models
```

Did you train all of the models?


## Q2
Now that you have all the trained models in a list, use sapply or map to create a matrix of predictions for the test set. You should end up with a matrix with length(mnist_27$test$y) rows and length(models).

```{r}
pred <-sapply(fits, function(x){
  predict(x, newdata = mnist_27$test)
})
dim(pred)

```


What are the dimensions of the matrix of predictions?
Rows? 200
Columns? 23

## Q3
Now compute accuracy for each model on the test set. Report the mean accuracy across all models.
```{r}
results <- colMeans(pred == mnist_27$test$y)
mean(results)
```
Answer: 0.8063043 Exp(0.807)

## Q4

Next, build an ensemble prediction by majority vote and compute the accuracy of the ensemble.

What is the accuracy of the ensemble? 0.845

```{r}
votes <- rowMeans(pred == "7")
y_hat <- ifelse(votes > 0.5, "7", "2")
mean(y_hat == mnist_27$test$y)
```

## Q5
In Q3, we computed the accuracy of each method on the training set and noticed that the individual accuracies varied.


How many of the individual methods do better than the ensemble? 2 **Expected 1**
Which individual methods perform better than the ensemble?  "svmRadial" & "svmRadialCost"  **Expected loclda**


```{r}
ind <- results > mean(y_hat == mnist_27$test$y)
sum(ind)
models[ind]
```


## Q6
It is tempting to remove the methods that do not perform well and re-do the ensemble. The problem with this approach is that we are using the test data to make a decision. However, we could use the accuracy estimates obtained from cross validation with the training data. Obtain these estimates and save them in an object. Report the mean accuracy of the new estimates.

What is the mean accuracy of the new estimates?  0.813 (Exp 0.812)

```{r}
results_hat <- sapply(fits, function(fit) min(fit$results$Accuracy))
mean(results_hat)
```

## Q7
Now let's only consider the methods with an estimated accuracy of greater than or equal to 0.8 when constructing the ensemble.

What is the accuracy of the ensemble now? 0.85

```{r}
ind <- results_hat >= 0.8
votes <- rowMeans(pred[,ind] == "7")
y_hat <- ifelse(votes>=0.5, 7, 2)
mean(y_hat == mnist_27$test$y)
```

