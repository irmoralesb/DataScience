---
title: "Comprehension Check: Linear Regression"
output: 
  github_document:
    pandoc_args: --webtex
---


## Q1
Create a data set using the following code:

```{r}
library(tidyverse)
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
```



We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models. First, set the seed to 1. Within a replicate loop,   
(1) partition the dataset into test and training sets of equal size using dat$y to generate your indices,   
(2) train a linear model predicting y from x,   
(3) generate predictions on the test set, and  
(4) calculate the RMSE of that model. Then, report the mean and standard deviation of the RMSEs from all 100 models.

Mean & SD:
```{r echo=FALSE}
library(caret)
set.seed(1)
results <- replicate(n,{
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
  train_set <-  dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  fit <- lm(y ~ x, data = train_set)
  y_hat <- predict(fit,newdata = test_set)
  sqrt(mean((y_hat-test_set$y)^2))
})

mean(results)
sd(results) 
```
Answer: 2.493618 & 0.1404267 (SD = 0.124?)

## Q2
Now we will repeat the exercise above but using larger datasets. Write a function that takes a size n, then (1) builds a dataset using the code provided in Q1 but with n observations instead of 100 and without the set.seed(1), (2) runs the replicate loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and (3) calculates the mean and standard deviation. Set the seed to 1 and then use sapply or map to apply this function to n <- c(100, 500, 1000, 5000, 10000).

Hint: You only need to set the seed once before running your function; do not set a seed within your function. Also be sure to use sapply or map as you will get different answers running the simulations individually due to setting the seed.


Mean and SD, 100:
```{r}

set.seed(1)
n <- c(100, 500, 1000, 5000, 10000)
results <- sapply(n, function(n){
	Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
	dat <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
		data.frame() %>% setNames(c("x", "y"))
	rmse <- replicate(100, {
		test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
		train_set <- dat %>% slice(-test_index)
		test_set <- dat %>% slice(test_index)
		fit <- lm(y ~ x, data = train_set)
		y_hat <- predict(fit, newdata = test_set)
		sqrt(mean((y_hat-test_set$y)^2))
	})
	c(avg = mean(rmse), sd = sd(rmse))
})
results

```
 

Mean & SD, 500:
```{r}
results[[1]]
results[[2]]

```
 
 
Mean & SD, 1000:
```{r}
results[[3]]
results[[4]]

```
 

Mean & SD, 5000:
```{r}
results[[5]]
results[[6]]

```
 

Mean & SD, 10000:
```{r}
results[[7]]
results[[8]]

```

## Q3
What happens to the RMSE as the size of the dataset becomes larger?

Answer:  On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases. 

## Q4

Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:
```{r}
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))

results <- replicate(n,{
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
  train_set <-  dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  fit <- lm(y ~ x, data = train_set)
  y_hat <- predict(fit,newdata = test_set)
  sqrt(mean((y_hat-test_set$y)^2))
})

mean(results)
sd(results) 

```

Note what happens to RMSE - set the seed to 1 as before.
I got 
Mean : 0.9167387 (mark as correct)
SD: 0.05226268 (mark as incorrect), the expected value was : 0.0624

This is the code form the exercise:
```{r}
set.seed(1)
rmse <- replicate(100, {
	test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
	train_set <- dat %>% slice(-test_index)
	test_set <- dat %>% slice(test_index)
	fit <- lm(y ~ x, data = train_set)
 	y_hat <- predict(fit, newdata = test_set)
	sqrt(mean((y_hat-test_set$y)^2))
})

mean(rmse)
sd(rmse)
```


## Q5

Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?

Answer: When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. correct


## Q6
Create a data set using the following code.
```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Note that y is correlated with both x_1 and x_2 but the two predictors are independent of each other, as seen by cor(dat).

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2 and both x_1 and x_2. Train a linear model for each.

Which of the three models performs the best (has the lowest RMSE)?

```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
  
```



## Q7
Report the lowest RMSE of the three models tested in Q6.
Answer: 0.307

## Q8
Repeat the exercise from q6 but now create an example in which x_1 and x_2 are highly correlated.
```{r}
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
	data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x_1, just x_2, and both x_1 and x_2.

Compare the results from q6 and q8. What can you conclude?

```{r}
set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))

fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

**Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.**

When the predictors are highly correlated with each other, adding addtional predictors does not improve the model substantially, thus RMSE stays roughly the same.