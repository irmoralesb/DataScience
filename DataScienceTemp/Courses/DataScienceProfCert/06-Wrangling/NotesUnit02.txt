Video 1

This module, we're going to define tidy data.
To help define tidy data, we go back to an example we showed in the data
visualization course, in which we applauded fertility data across time
for two countries--
South Korea and Germany.
Here's the plot.
To make the plot, we used this subset of the data
and get writing this piece of code.

With the data in this format, we can quickly
make the desired plot using this very simple piece of code.
One reason this code works seamlessly is because the data is tidy.
Each point in the plot is represented by a row in the table.
This brings us to the definition of tidy data.
Each row represents one observation and the columns
represent the different variables that we have data on for those observations.
If we go back to the original data provided by Gapminder,
we see that it does not start out tidy.
We include an example file with the data shown in this graph,
mimicking the way it was originally saved in a spreadsheet.
You can get to the file like this.
After running that code, the object wide_data
includes the same information as the object tidy_data,
except it is in a different format--
a wide format.
Here are the first nine columns of this wide data.
Let's go over two important differences between the wide and tidy formats.
First, in the wide format, each row includes several observations.
Second, one of the variables-- the year--
is stored in the header.
The [INAUDIBLE] plot code we introduced earlier, no longer
works if we feed it the wide data.
For one, there is no year available.
So to use the tidyverse we need to wrangle this data into tidy format.

data("gapminder")
tidy_data <- gapminder %>%
	filter(country %in% c("South Korea", "Germany")) %>%
	select(country, year, fertility)
head(tidy_data)

tidy_data %>%
	ggplot(aes(year,fertility,color= country)) +
	geom_point()


path >- system.file("extdata",package="dslabs")
filename <- file.path(path, "fertility-two-countries-example-csv")
wide_data <- read_csv(filename)
# This is in a wide format
select (wide_data, country, ´1960´:´1967´)

#Wide Format
1) each row includes several observations
2) one of the variables is stored in the header

So to use the tidyverse we need to wrangle this data into tidy format.


Video 2
We've learned that having data in tidy format
is what makes a tidy verse flow.
After the first step in the data analysis process, importing data,
a common next step is to reshape the data into a form that
facilitates the rest of the analysis.
The tidyr package includes several functions
that are useful for tidying data.
This package is included in the tidy verse.
One of the most used functions in the package
is gather, which converts y data into tidy data.
We'll get to the first and second argument of gather soon,
but let's describe the third argument.
The third argument of the gather function
specifies the columns that will be gathered.
The default behavior for the gather function is to gather all the columns.
So in most cases, we have to specify the columns.
In the example we've been examining, we want to gather the columns 1960, 1961,
up to 2015.
Those are the column names.
Now let's explain what the first argument of the gather function does.
The first argument sets the name of the column
that will hold the variable that are currently kept in the y data column
names.
In our case, it makes sense to set the name of this column to year,
but we can name it anything.
The second argument sets the column name for the column
that will hold the values in the column cells.
In this case, we'll call it fertility since that's
the data that is in those cells.
Know that nowhere in this file does it tell us that this is fertility data.
We know this from the file name.
This is not the best way to store data but it's
the way this data was given to us.
Now the gathering code looks like this.
We're going to create a new tidy data sets object, call it new
underscore tidy underscore data, and now all we
do is apply the gather function to the y data.
We can see that the data have been converted
to tidy format would columns year and fertility.
Look at the first six rows.
Note that the only column that was not gathered was the countries column.
That's because we asked for all the other ones to be gathered.
So a somewhat quicker way to write this code
is to specify which columns not to gather,
rather than all the columns that will be gathered.
So the code will look simply like this.
The object looks a lot like the original tidy data we showed earlier.
There's just one minor difference.
Can you spot it?
Look at the data type for the Year column.
It's a integer in our original tidy data table.
In our new tidy data, the one we just gathered, it's a character.
The gather function assumes that column names are characters,
so we need a bit more wrangling before we're ready to make a plot.
We need to convert this column to numbers.
We can use as numeric if we want, but the gathered function actually
has an argument for that.
It's the convert argument.
So the code would be like this.
And once we do this, the class of the Year column is an integer.
Now that the data is tidy, we can use the same ggplot commands
to generate the plot we saw earlier.
Like this.
So we've shown how to gather wide data into tidy data.
Now, as we will see in later example, it is
sometimes useful for data wrangling purposes
to convert tiny data into the wide format data.
We often, use this as an intermediate step in tidying up data.
The spread function is basically, the inverse of gather.
The first argument tells spread which variables
will be used as the column names.
The second argument specifies which variables to use to fill out the cells.
So the code would look like this.
This converts tiny data back into the wide format--
as you can see here.
This diagram can help you remember, how these two functions work.

Video 3
The data wrangling examples we have shown
are actually quite simple compared to what is usually required.
In our example spreadsheets included in the dslabs package,
we include an example that is slightly more complicated.
And we're going to use it to show a slightly more realistic example.
This file includes two variables, life expectancy, as well as fertility.
However, the way it is stored is not tidy.
And as we will explain, not optimal at all.
You can read in the data using this piece of code.
When we look at this table, we can see that it is in y format.
Also note that there are values for two variables with the column
names encoding which column represents which variable.
We can start the data wrangling with the gather function,
but we should no longer use the column name Year for the new columns,
since it also contains the variable type.
We will call it key.
That's the default of this function.
So we write this piece of code to gather the data.
And the result is not exactly what we refer to as tidy,
since each observation is associated with two, not one, row.
We want to have the value from the two variables, fertility and life
expectancy, as two separate columns.
The first challenge to achieve this is to separate the key column
into the year and the variable type.
Note that the entries in this column separate the year
from the variable name using an underscore.
You can see it here.
Encoding multiple variables in a column name
is such a common problem that the reader package
includes a function to separate these columns into two or more functions.
The function is called separate.
Apart from the data, the separate function takes three arguments--
the name of the column to be separated, the names
to be used for the new columns, and the character that separates the variables.
So a first attempt would be to write this piece of code.
Now, because the underscore is the default separator,
we can actually simply write code like this,
without telling separate what the separator is.
However, we run into a problem.
Note that we have received a warning, too many values at 112 locations,
and that the life expectancy variable is truncated to just life.
This is because the underscore is used to separate life
and expectancy in the name, not just to separate year and the variable name.
So what can we do?
We can add a third column to catch this and let
the separate function know which column to fill in with missing values--
NAs, in this case--
when there is no third value.
Here, in this piece of code, we tell it to fill the column on the right.

However, if we read the separate file, we
find that a better approach is to merge the last two
variables when there's an extra separation
using the argument extra, like this.

However, we're not done yet.
We need to create a column for each variable.
As we've learned, the spread function can do this.
So now, to create tidy data, we're actually using the spread function.
So we write this piece of code, and when we run it,
we now get a fertility and a life expectancy column.
Now, it is also sometimes useful to do the inverse of separate,
which is to unite two columns into one.
So although this is not an optimal approach,
we could have done the following to achieve the same result.
We use separate like this, and then we use this code
that unites the two columns into one.
And then we spread the columns with this code.
So this is clearly not as efficient, but it
provides an example of where you would use the unite function.

Video 4
The information we need for a given analysis may not
be in just one table.
For example, when forecasting elections, we
use the function left underscore join to combine
the information from two tables.
Here, we use a simple example to illustrate the general challenge
of combining tables.
Suppose we want to explore the relationship between population
size for US states, which we have in this table,
and electoral votes, which we have in this one.
Notice that just joining these two tables together like this
will not work since the order of the states is not quite the same.
We can see this by typing this code.
We see that the column names for the state names are not the same.
The join functions in the dplyr package, which are based on the SQL joins,
make sure that the tables are combined so that matching rows are together.
The general idea is that one needs to identify
one or more columns that contain the information needed
to match the two tables.
Then, a new table with the combined information is returned.
Note what happens if we join the two tables by state using left join.
The data has been successfully joined, and we can now
make a plot to explore the relationship we're
interested in by using this simple code.
The plot shows that there is a relationship that's close to linear,
with about two electoral votes for every million people.
But with smaller states getting a higher ratio.
Now, in practice, it is not always the case that each row in one table
has a matching row in the other.
For this reason, we have several different ways to join.
To illustrate this challenge, we're going
to take a subset of the two tables that we've been using.
We're going to create two objects.
Tab one is going to be a subset of our first table,
and tab two is going to be a subset of our second table.
However, the states contained in the two tables are going to differ.
We're going to use these two tables as examples of the different join
functions.
Let's start with left join.
Suppose we want a table like tab one, but adding
electoral votes to whatever states we have available in tab one.
For this, we use left join, with tab one as the first argument, like this.
Note that NAs are added to two states in tab one
that are not appearing in tab two.
Also note that this function, as well as all the other joins,
can receive the first argument through the pipe, like this.

If instead of a table like tab one we want one like tab two,
we can use the right join, like this.
Notice that now the NAs are in the columns coming from tab one.
Now, if we want to keep only the rows that have information in both tables,
we use inner join.
You can think of this as an intersection.
So we type inner join tab one, tab two, and we get the following tables.
You know that this one doesn't have NAs.
Now, if you want to keep all the rows and fill in the missing parts with NAs,
we can use full join.
You can think of this as a union.
The code is simply like this.
There's two more join functions we're going to go over.
They're a little bit different because they don't actually join the tables.
Instead, they let you keep parts of one table,
depending on what's in the other.
The semi join function lets us keep the part of the first table for which we
have information in the second.
It does not add the columns of the second.
You can see it by typing this.
The function anti join is the opposite of semi join.
It keeps the elements of the first table for which there
is no information in the second.
So if we type anti join tab one, tab two, we get this small little table.
To learn more about how the joins work and to remember them,
you can look at this diagram.

Video 5
Although we have not yet used it in this series,
another common way in which data sets are combined are by binding them.
Regardless of the row order.
Unlike the join functions, the binding functions
do not try to match by a variable, but rather just combine the data sets.
If the data sets don't match by the appropriate dimension
we will obtain an error.
The dplyr function bind_cols binds two objects by putting the columns of each
together in a table.
For example, if we quickly want to make a data
frame consisting of just numbers, we can use something like this.
The function requires that we assign names to the columns.
Here we chose A and B, but it could be anything.
Note that there's an r-based function, cbind, that performs the same function
but creates objects other than tibbles, either matrices or data frames,
something else.
Bind_cols can also bind data frames.
For example, here we break up the tab data frame
and then bind them back together.
So we break it up into three with different columns,
and then we simply bind them back together.
So you can see that we get the original data again.
The bind_rows is similar, but binds rows instead of columns.
We'll show a simple example where we take the first two
rows, and the third and fourth rows, and then bind them together to get rows 1
through 4.
This one is based on an r-based function called rbind.


Video 6
Another set of commands useful for combining
data are the set operators.
When applied to vectors, these behave as their name
suggests-- union, intersect, et cetera.
And we're going to see examples soon.
However, if the tidyverse, or, more specifically, dplyr is loaded,
these functions can be used on data frames, as opposed to just on vectors.
Let's start with intersect.
You can take the intersection of numeric vectors--
for example, like this--
or character vectors.
It's simply taking the intersection.
But with dplyr loaded, we can also do this for tables.
It'll take the intersection of rows for tables having the same column names.
So if we take the first five rows of tab and rows three through seven of tabs,
and we take the intersection, it will give us rows three, four, and five,
which you can see here.
Similarly, union takes the union.
If you apply it to vectors, you get the union like this.
But with dplyr loaded, we can also do this for tables having the same column
names.
So if we take those two tables we defined previously and take the union,
we now get rows one through seven, the rows that are in common, the union.
We can also take set differences using the function setdiff.
Unlike intersect and union, this function is not symmetric.
For example, note that you get two different answers
if you switch the arguments.
You can see that using this example.
And again, with dplyr loaded, we can apply this to data frames.
Look what happens when we take the setdiff of tab one and tab two.
Finally, the function setequal tells us if two sets
are the same regardless of order.
So for example, if I do set equals of one through five and one through six,
I get false, because they're not the same vectors.
But if I take set equals of one through five and five through one,
I get true, because if you ignore order, these are the same vectors.
With dplyer loaded, we can use this on data frames, as well.
When applied to data frames that are not equal, regardless of order,
it provides a useful message letting us know how the sets are different.
Look what happens if I ask if tab one and tab two are set equal.
We get false, and we also get told which rows are different
and which ones are in x and not in y, and which ones are in y and not in x.
This can be quite useful.
End of transcript. Skip to the start.

Video 7
The data we need to answer questions are not always
in a spreadsheet ready for us to read.
For example, the US murders data set we used in R basics
course originally came from this Wikipedia page.
You can see the data table when you visit the page.
But unfortunately, there is no link to the data file.
To make the data frame, we load it using data frames or the CSV files that we
made available through DS Labs--
we have to do some web scraping.
Web scraping or web harvesting are the terms
used to describe the process of extracting data from a website.
The reason we can do this is because the information used by a browser
to render web pages is received as text from a server.
The text is computer code written in HyperText Markup Language or HTML.
To see the code for a web page, you can actually visit the page on your browser
and then view the code.
Different browsers have different ways of doing this.
In Chrome you can click on View Source to see it.
Because this code is accessible, we can download the HTML files,
import it into R, and then write programs to extract
the information we need from the page.
However, once we look at HTML code, this might seem like a daunting task.
But we will show you some convenient tools to facilitate the process.
To get an idea of how web scraping works,
take a look at these few lines of code from the Wikipedia page
that provides the Us murders data.
You can actually see the data if you look hard enough.
We can also see a pattern of how it's stored.
If you know HTML, you know what these patterns are,
and you can leverage this knowledge to extract what we need.
We can also take advantage of a language widely used
to make web pages look pretty called Cascading Style Sheets, or CSS.
We say more about this in a later video.
Although we provide tools that make it possible to scrape data without knowing
HTML, for data scientists, it is quite useful to learn some HTML and some CSS.
Not only does this improve your scraping skills,
but it might come in handy if you're creating a web
page to showcase your work.
There are plenty of online courses and tutorials for learning these,
so we don't cover them.
We're going to provide some links in the course material.
The package we're going to learn to do web scraping is part of the tidyverse,
and it's called rvest.
The first step using this package is to import the web page
into R. We can do this using this code.
We load the library, we denote the URL of the page we want to read,
and then use the read_html function to read it into the object h.
We can call it anything we want--
we decided to call it h here.
Note that the entire murders in the US Wikipedia page
is now contained in this object.
The class of this object is actually XML document.
The harvest package is actually more general.
It handles XML documents, not just HTML documents.
XML is a general markup language-- that's what the XML stands for.
This language can be used to represent any kind of data.
HTML is a specific type of XML, specifically developed
for representing web pages.
Here we focus on HTML documents.
Now, we have this object h, how do we extract a table from the object?
If we print h, we don't really see much.
Here we know that the information is stored in an HTML table.
You can see this in a line of code of the HTML document we showed earlier.
The specific line is this one.
The different parts of HTML documents often
define messages between the two symbols less than and greater than.
These are referred to as nodes. .
The rvest package includes functions to extract nodes from HTML documents.
The function html_nodes, plural, extracts all nodes of that type.
And html_node extracts just the first node of that type.
To extract the first table, we can use this very simple code.
Now, instead of the entire web page, we just have the HTML code for that table.
And we can see it by printing [? out ?] [? tab. ?] We are not quite there yet
though, because this is clearly not a tidy data set.
It's not even a data frame.
In the code we just showed, you can definitely see a pattern,
and writing code to extract just the data is very doable.
In fact rvest includes a function precisely for this--
for converting HTML tables into data frames.
Here's the code that you would use.
If you use this function, you will extract the table from the HTML table.
And now you get a data frame.
We are now much closer to having a usable data table.
Let's change the names of the columns, which are a little bit long,
and then take a look. / You can see that we already
have a data frame very close to what we want.
However, we still have some data wrangling to do.
For example, notice that some of the columns that are supposed to be numbers
are actually characters.
And what makes it even worse is that some of them have commas,
so it makes it harder to convert to numbers.
Before we continue with this, we're going to learn a little bit more
about general approaches to extracting information from websites.
Then we'll get back to our example.

Video 8 ( No Video just text)
The default look of webpage made with the most basic HTML is quite unattractive. The aesthetically pleasing pages we see today are made using CSS. CSS is used to add style to webpages. The fact that all pages for a company have the same style is usually a result that they all use the same CSS file. The general way these CSS files work is by defining how each of the elements of a webpage will look. The title, headings, itemized lists, tables, and links, for example, each receive their own style including font, color, size, and distance from the margin, among others.

To do this CSS leverages patterns used to define these elements, referred to as selectors. An example of pattern we used in a previous video is table but there are many many more. If we want to grab data from a webpage and we happen to know a selector that is unique to the part of the page, we can use the html_nodes function.

However, knowing which selector to use can be quite complicated. To demonstrate this we will try to extract the recipe name, total preparation time, and list of ingredients from this guacamole recipe. Looking at the code for this page, it seems that the task is impossibly complex. However, selector gadgets actually make this possible. SelectorGadget is piece of software that allows you to interactively determine what CSS selector you need to extract specific components from the webpage. If you plan on scraping data other than tables, we highly recommend you install it. A Chrome extension is available which permits you to turn on the gadget highlighting parts of the page as you click through, showing the necessary selector to extract those segments.

For the guacamole recipe page we already have done this and determined that we need the following selectors:

h <- read_html("http://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time <- h %>% html_node(".o-RecipeInfo__a-Description--Total") %>% html_text()
ingredients <- h %>% html_nodes(".o-Ingredients__a-ListItemText") %>% html_text()

You can see how complex the selectors are. In any case we are now ready to extract what we want and create a list:

guacamole <- list(recipe, prep_time, ingredients)
guacamole

Since recipe pages from this website follow this general layout, we can use this code to create a function that extracts this information:

get_recipe <- function(url){
    h <- read_html(url)
    recipe <- h %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
    prep_time <- h %>% html_node(".o-RecipeInfo__a-Description--Total") %>% html_text()
    ingredients <- h %>% html_nodes(".o-Ingredients__a-ListItemText") %>% html_text()
    return(list(recipe = recipe, prep_time = prep_time, ingredients = ingredients))
}

and then use it on any of their webpages:

get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")

There are several other powerful tools provided by rvest. For example, the functions html_form, set_values, and submit_form permit you to query a webpage from R. This is a more advanced topic not covered here.
