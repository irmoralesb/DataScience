Comprehension Check: Linear Regression
================

Q1
--

Create a data set using the following code:

``` r
library(tidyverse)
```

    ## Warning: package 'tidyverse' was built under R version 3.5.2

    ## -- Attaching packages ------------------------------------------------------ tidyverse 1.2.1 --

    ## v ggplot2 3.0.0     v purrr   0.2.5
    ## v tibble  1.4.2     v dplyr   0.7.6
    ## v tidyr   0.8.1     v stringr 1.3.1
    ## v readr   1.1.1     v forcats 0.3.0

    ## -- Conflicts --------------------------------------------------------- tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))
```

We will build 100 linear models using the data above and calculate the mean and standard deviation of the combined models. First, set the seed to 1. Within a replicate loop,
(1) partition the dataset into test and training sets of equal size using dat$y to generate your indices,
(2) train a linear model predicting y from x,
(3) generate predictions on the test set, and
(4) calculate the RMSE of that model. Then, report the mean and standard deviation of the RMSEs from all 100 models.

Mean & SD:

    ## Warning: package 'caret' was built under R version 3.5.3

    ## Loading required package: lattice

    ## Warning: package 'lattice' was built under R version 3.5.2

    ## 
    ## Attaching package: 'caret'

    ## The following object is masked from 'package:purrr':
    ## 
    ##     lift

    ## [1] 2.488661

    ## [1] 0.1243952

Answer: 2.493618 & 0.1404267 (SD = 0.124?)

Q2
--

Now we will repeat the exercise above but using larger datasets. Write a function that takes a size n, then (1) builds a dataset using the code provided in Q1 but with n observations instead of 100 and without the set.seed(1), (2) runs the replicate loop that you wrote to answer Q1, which builds 100 linear models and returns a vector of RMSEs, and (3) calculates the mean and standard deviation. Set the seed to 1 and then use sapply or map to apply this function to n &lt;- c(100, 500, 1000, 5000, 10000).

Hint: You only need to set the seed once before running your function; do not set a seed within your function. Also be sure to use sapply or map as you will get different answers running the simulations individually due to setting the seed.

Mean and SD, 100:

``` r
set.seed(1)
n <- c(100, 500, 1000, 5000, 10000)
res <- sapply(n, function(n){
    Sigma <- 9*matrix(c(1.0, 0.5, 0.5, 1.0), 2, 2)
    dat <- MASS::mvrnorm(n, c(69, 69), Sigma) %>%
        data.frame() %>% setNames(c("x", "y"))
    rmse <- replicate(100, {
        test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
        train_set <- dat %>% slice(-test_index)
        test_set <- dat %>% slice(test_index)
        fit <- lm(y ~ x, data = train_set)
        y_hat <- predict(fit, newdata = test_set)
        sqrt(mean((y_hat-test_set$y)^2))
    })
    c(avg = mean(rmse), sd = sd(rmse))
})
res
```

    ##          [,1]       [,2]       [,3]       [,4]       [,5]
    ## avg 2.4977540 2.72095125 2.55554451 2.62482800 2.61844227
    ## sd  0.1180821 0.08002108 0.04560258 0.02309673 0.01689205

Mean & SD, 500:

``` r
mean(results[[2]])
```

    ## [1] 2.624561

``` r
sd(results[[2]])
```

    ## [1] NA

Mean & SD, 1000:

``` r
mean(results[[3]])
```

    ## [1] 2.543693

``` r
sd(results[[3]])
```

    ## [1] NA

Mean & SD, 5000:

``` r
mean(results[[4]])
```

    ## [1] 2.675355

``` r
sd(results[[4]])
```

    ## [1] NA

Mean & SD, 10000:

``` r
mean(results[[5]])
```

    ## [1] 2.516285

``` r
sd(results[[5]])
```

    ## [1] NA

Q3
--

What happens to the RMSE as the size of the dataset becomes larger?

Answer: On average, the RMSE does not change much as n gets larger, but the variability of the RMSE decreases.

Q4
--

Now repeat the exercise from Q1, this time making the correlation between x and y larger, as in the following code:

``` r
set.seed(1)
n <- 100
Sigma <- 9*matrix(c(1.0, 0.95, 0.95, 1.0), 2, 2)
dat <- MASS::mvrnorm(n = 100, c(69, 69), Sigma) %>%
    data.frame() %>% setNames(c("x", "y"))

results <- replicate(n,{
  test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
  train_set <-  dat %>% slice(-test_index)
  test_set <- dat %>% slice(test_index)
  fit <- lm(y ~ x, data = train_set)
  y_hat <- predict(fit,newdata = test_set)
  sqrt(mean((y_hat-test_set$y)^2))
})

mean(results)
```

    ## [1] 0.9167387

``` r
sd(results) 
```

    ## [1] 0.05226268

Note what happens to RMSE - set the seed to 1 as before. I got Mean : 0.9167387 (mark as correct) SD: 0.05226268 (mark as incorrect), the expected value was : 0.0624

This is the code form the exercise:

``` r
set.seed(1)
rmse <- replicate(100, {
    test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
    train_set <- dat %>% slice(-test_index)
    test_set <- dat %>% slice(test_index)
    fit <- lm(y ~ x, data = train_set)
    y_hat <- predict(fit, newdata = test_set)
    sqrt(mean((y_hat-test_set$y)^2))
})

mean(rmse)
```

    ## [1] 0.9099808

``` r
sd(rmse)
```

    ## [1] 0.06244347

Q5
--

Which of the following best explains why the RMSE in question 4 is so much lower than the RMSE in question 1?

Answer: When we increase the correlation between x and y, x has more predictive power and thus provides a better estimate of y. correct

Q6
--

Create a data set using the following code.

``` r
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
    data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Note that y is correlated with both x\_1 and x\_2 but the two predictors are independent of each other, as seen by cor(dat).

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x\_1, just x\_2 and both x\_1 and x\_2. Train a linear model for each.

Which of the three models performs the best (has the lowest RMSE)?

``` r
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.25, 0.75, 0.25, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
    data.frame() %>% setNames(c("y", "x_1", "x_2"))

set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.600666

``` r
fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.630699

``` r
fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.3070962

Q7
--

Report the lowest RMSE of the three models tested in Q6. Answer: 0.307

Q8
--

Repeat the exercise from q6 but now create an example in which x\_1 and x\_2 are highly correlated.

``` r
set.seed(1)
Sigma <- matrix(c(1.0, 0.75, 0.75, 0.75, 1.0, 0.95, 0.75, 0.95, 1.0), 3, 3)
dat <- MASS::mvrnorm(n = 100, c(0, 0, 0), Sigma) %>%
    data.frame() %>% setNames(c("y", "x_1", "x_2"))
```

Set the seed to 1, then use the caret package to partition into a test and training set of equal size. Compare the RMSE when using just x\_1, just x\_2, and both x\_1 and x\_2.

Compare the results from q6 and q8. What can you conclude?

``` r
set.seed(1)
test_index <- createDataPartition(dat$y, times = 1, p = 0.5, list = FALSE)
train_set <- dat %>% slice(-test_index)
test_set <- dat %>% slice(test_index)

fit <- lm(y ~ x_1, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.6592608

``` r
fit <- lm(y ~ x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.640081

``` r
fit <- lm(y ~ x_1 + x_2, data = train_set)
y_hat <- predict(fit, newdata = test_set)
sqrt(mean((y_hat-test_set$y)^2))
```

    ## [1] 0.6597865

**Adding extra predictors can improve RMSE substantially, but not when the added predictors are highly correlated with other predictors.**

When the predictors are highly correlated with each other, adding addtional predictors does not improve the model substantially, thus RMSE stays roughly the same.
